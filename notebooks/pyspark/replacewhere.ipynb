{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "The Delta Lake [`replaceWhere`](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) option allows users to selectively apply updates to specific data partitions rather than to full lakes, which may result in significant speed gains. This notebook briefly illustrates the usage of `replaceWhere` option. For more details, see:\n",
    "- [Selectively updating Delta partitions with replaceWhere](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) (this notebook will be following the example from this blog)\n",
    "- [Selectively overwrite data with Delta Lake](https://docs.databricks.com/delta/selective-overwrite.html)\n",
    "- [Table batch reads and writes: overwrite](https://docs.delta.io/latest/delta-batch.html#overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple replaceWhere example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"a\", 1),\n",
    "        (\"b\", 2),\n",
    "        (\"c\", 3),\n",
    "    ]\n",
    ").toDF(\"letter\", \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 21:55:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").save(\"tmp/my_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     x|     7|\n",
      "|     y|     8|\n",
      "|     z|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/my_data\").orderBy(col(\"number\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"x\", 7),\n",
    "        (\"y\", 8),\n",
    "        (\"z\", 9),\n",
    "    ]\n",
    ").toDF(\"letter\", \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df2.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"number >= 2\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"tmp/my_data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     x|     7|\n",
      "|     y|     8|\n",
      "|     z|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/my_data\").orderBy(col(\"number\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple replaceWhere example with partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"aa\", 11),\n",
    "        (\"bb\", 22),\n",
    "        (\"aa\", 33),\n",
    "        (\"cc\", 33),\n",
    "    ]\n",
    ").toDF(\"patient_id\", \"medical_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").partitionBy(\"medical_code\").save(\"tmp/patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/patients\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "├── \u001b[01;34mmedical_code=11\u001b[0m\n",
      "│   └── \u001b[00mpart-00002-49a164ed-7590-4d4c-8216-bc1a6947ff3b.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mmedical_code=22\u001b[0m\n",
      "│   └── \u001b[00mpart-00004-8364a37a-f5d8-4cfa-8daa-065b5760bedd.c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[01;34mmedical_code=33\u001b[0m\n",
      "    ├── \u001b[00mpart-00007-522512ed-d6ad-4c3f-996d-5a737b12030b.c000.snappy.parquet\u001b[0m\n",
      "    └── \u001b[00mpart-00009-d708e56b-0d87-4545-b3b7-9fc4d3053560.c000.snappy.parquet\u001b[0m\n",
      "\n",
      "4 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|patient_id|medical_code|\n",
      "+----------+------------+\n",
      "|        aa|          11|\n",
      "|        bb|          22|\n",
      "|        aa|          33|\n",
      "|        cc|          33|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"tmp/patients\")\n",
    "    .orderBy(col(\"medical_code\").asc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"dd\", 33),\n",
    "        (\"f\", 33),\n",
    "    ]\n",
    ").toDF(\"patient_id\", \"medical_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df2.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"medical_code = '33'\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"medical_code\")\n",
    "    .save(\"tmp/patients\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|patient_id|medical_code|\n",
      "+----------+------------+\n",
      "|        aa|          11|\n",
      "|        bb|          22|\n",
      "|        dd|          33|\n",
      "|         f|          33|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"tmp/patients\")\n",
    "    .orderBy(col(\"medical_code\").asc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|  country|continent|\n",
      "+----------+---------+---------+---------+\n",
      "|   Ernesto|  Guevara|Argentina|     null|\n",
      "|  Vladimir|    Putin|   Russia|     null|\n",
      "|     Maria|Sharapova|   Russia|     null|\n",
      "|     Bruce|      Lee|    China|     null|\n",
      "|      Jack|       Ma|    China|     null|\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.options(header=\"True\", charset=\"UTF8\")\n",
    "    .csv(\"../../data/people_countries.csv\")\n",
    "    .withColumn(\"continent\", lit(None).cast(StringType()))\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition on Country\n",
    "Now we'll repartition the DataFrame on `country` and write it to disk in the Delta Lake format, partitioned by `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 13:28:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "deltaPath = \"../../data/people_countries_delta/\"\n",
    "\n",
    "(\n",
    "    df.repartition(col(\"country\"))\n",
    "    .write.partitionBy(\"country\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to add `continent` values to a DataFrame based on the value of `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "def withContinent(df):\n",
    "    return df.withColumn(\n",
    "        \"continent\",\n",
    "        when(col(\"country\") == \"Russia\", \"Europe\")\n",
    "        .when(col(\"country\") == \"China\", \"Asia\")\n",
    "        .when(col(\"country\") == \"Argentina\", \"South America\"),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where `replaceWhere` comes in. Suppose we only want to populate the `continent` column when `country == 'China'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "df = df.where(col(\"country\") == \"China\").transform(withContinent)\n",
    "\n",
    "(\n",
    "    df.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"country = 'China'\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|country  |continent|\n",
      "+----------+---------+---------+---------+\n",
      "|Bruce     |Lee      |China    |Asia     |\n",
      "|Jack      |Ma       |China    |Asia     |\n",
      "|Ernesto   |Guevara  |Argentina|null     |\n",
      "|Vladimir  |Putin    |Russia   |null     |\n",
      "|Maria     |Sharapova|Russia   |null     |\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=China/part-00000-2f823649-f7af-45e7-95cc-fce354972434.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 1002,\n",
      "        \"modificationTime\": 1687544927388,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"first_name\\\":\\\"Bruce\\\",\\\"last_name\\\":\\\"Lee\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Jack\\\",\\\"last_name\\\":\\\"Ma\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=China/part-00000-5b20d31c-1a49-47f0-a5b1-0f2e5b422753.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1687544926730,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 929\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    \"../../data/people_countries_delta/_delta_log/00000000000000000001.json\", \"r\"\n",
    ") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if \"add\" in data or \"remove\" in data:\n",
    "            print(json.dumps(data, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only the `country=China/part-00000-87aebbc2-aff3-4bd6-b369-aa9aacbb93be.c000.snappy.parquet` file was modified. The other partitions were not.\n",
    "\n",
    "For more details, read the [blog post](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-340-delta-240",
   "language": "python",
   "name": "pyspark-340-delta-240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
